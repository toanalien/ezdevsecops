---
title: "Ph·∫ßn 2: C√†i ƒë·∫∑t v√† c·∫•u h√¨nh OpenClaw"
date: 2026-02-15
draft: false
description: "H∆∞·ªõng d·∫´n chi ti·∫øt c√†i ƒë·∫∑t OpenClaw daemon v·ªõi Ollama local LLM, c·∫•u h√¨nh multi-provider routing, setup systemd services v√† expose web UI qua Caddy"
categories: ["AI Assistant"]
tags: ["openclaw", "ollama", "llm", "systemd", "caddy"]
series: ["OpenClaw Personal Assistant"]
weight: 2
mermaid: true
---

## Gi·ªõi thi·ªáu

Sau khi chu·∫©n b·ªã xong h·∫° t·∫ßng VPS, gi·ªù ch√∫ng ta s·∫Ω c√†i ƒë·∫∑t OpenClaw v√† c√°c th√†nh ph·∫ßn c·ªët l√µi. B√†i vi·∫øt n√†y h∆∞·ªõng d·∫´n deploy m·ªôt AI assistant ho√†n ch·ªânh v·ªõi local LLM v√† cloud fallback.

**B·∫°n s·∫Ω h·ªçc ƒë∆∞·ª£c:**

- ‚úì C√†i ƒë·∫∑t Ollama v√† pull LLM models (qwen2.5:7b, llama3.1:8b)
- ‚úì T·∫°o systemd service cho Ollama daemon
- ‚úì C√†i ƒë·∫∑t OpenClaw qua official installer
- ‚úì C·∫•u h√¨nh multi-LLM provider routing
- ‚úì Setup OpenClaw systemd service v·ªõi auto-restart
- ‚úì Expose web UI qua Caddy ho·∫∑c direct access
- ‚úì Verify v√† troubleshoot to√†n b·ªô h·ªá th·ªëng

**ƒêi·ªÅu ki·ªán ti√™n quy·∫øt:**

- Ho√†n th√†nh [Ph·∫ßn 1: Chu·∫©n b·ªã h·∫° t·∫ßng VPS](../01-infrastructure-preparation/)
- User `openclaw` ƒë√£ t·∫°o v·ªõi sudo privileges
- Node.js 22+ v√† pnpm ƒë√£ c√†i ƒë·∫∑t
- (T√πy ch·ªçn) API key c·ªßa Anthropic/OpenAI cho cloud fallback

## Ki·∫øn tr√∫c t·ªïng quan

{{< mermaid >}}
graph TB
    User[User Browser/API]
    Caddy[Caddy :443]
    OpenClaw[OpenClaw Gateway :18789]
    Ollama[Ollama :11434]
    Anthropic[Anthropic API]
    Workspace[Workspace Files]

    User -->|HTTPS| Caddy
    Caddy -->|Proxy| OpenClaw
    OpenClaw -->|Primary| Ollama
    OpenClaw -->|Fallback| Anthropic
    OpenClaw -->|Read/Write| Workspace

    subgraph VPS - openclaw user
        OpenClaw
        Ollama
        Workspace
    end

    subgraph External
        Anthropic
    end

    style OpenClaw fill:#FF6B6B
    style Ollama fill:#4ECDC4
    style Anthropic fill:#FFE66D
{{< /mermaid >}}

**LLM Routing Strategy:**

- **Local Ollama** - Primary cho t·∫•t c·∫£ tasks (free, private)
- **Anthropic API** - Fallback khi task ph·ª©c t·∫°p ho·∫∑c Ollama fail
- **Token limit aware** - Auto switch d·ª±a v√†o context size

## B∆∞·ªõc 1: C√†i ƒë·∫∑t Ollama

Ollama l√† runtime ƒë·ªÉ ch·∫°y LLM models locally v·ªõi inference t·ªëi ∆∞u.

### Download v√† c√†i ƒë·∫∑t

```bash
# Switch sang user openclaw
su - openclaw

# Download Ollama installer
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version
```

**Output mong ƒë·ª£i:**

```
ollama version is 0.4.5
```

### Test Ollama CLI

```bash
# Start Ollama server (t·∫°m th·ªùi)
ollama serve &

# List available models (r·ªóng ban ƒë·∫ßu)
ollama list

# Stop server
pkill ollama
```

## B∆∞·ªõc 2: Pull LLM Models

### Ch·ªçn models ph√π h·ª£p

| Model | Size | RAM c·∫ßn | Use case |
|-------|------|---------|----------|
| qwen2.5:7b | 4.7GB | 6GB+ | General purpose, code |
| llama3.1:8b | 4.9GB | 6GB+ | Reasoning, conversation |
| codestral:22b | 13GB | 16GB+ | Advanced coding (n·∫øu ƒë·ªß RAM) |
| gemma2:2b | 1.6GB | 3GB+ | Lightweight fallback |

{{< callout type="info" >}}
**Khuy·∫øn ngh·ªã cho VPS 4GB RAM:** Pull c·∫£ qwen2.5:7b v√† llama3.1:8b. OpenClaw s·∫Ω load model theo nhu c·∫ßu.
{{< /callout >}}

### Pull models

```bash
# Start Ollama server trong tmux (download l√¢u)
tmux new -s ollama
ollama serve

# M·ªü terminal m·ªõi
tmux split-window -h

# Pull primary model (4-5 ph√∫t)
ollama pull qwen2.5:7b

# Pull secondary model
ollama pull llama3.1:8b

# (T√πy ch·ªçn) Pull lightweight model cho low-memory fallback
ollama pull gemma2:2b

# Verify downloaded models
ollama list
```

**Output mong ƒë·ª£i:**

```
NAME                ID              SIZE      MODIFIED
qwen2.5:7b          a67f5c0c89e7    4.7 GB    2 minutes ago
llama3.1:8b         f66f3c5c89e7    4.9 GB    4 minutes ago
gemma2:2b           1a2b3c4d5e6f    1.6 GB    6 minutes ago
```

{{< callout type="tip" >}}
**D√πng tmux ƒë·ªÉ tr√°nh timeout:** Download model qua SSH c√≥ th·ªÉ b·ªã disconnect. `tmux attach -t ollama` ƒë·ªÉ reconnect session.
{{< /callout >}}

### Test model inference

```bash
# Test chat v·ªõi qwen2.5
ollama run qwen2.5:7b "Vi·∫øt h√†m Python t√≠nh fibonacci"

# Test v·ªõi llama3.1
ollama run llama3.1:8b "Explain Docker in Vietnamese"

# Exit chat: /bye
```

## B∆∞·ªõc 3: T·∫°o Ollama systemd service

ƒê·ªÉ Ollama t·ª± ƒë·ªông start khi boot v√† restart n·∫øu crash.

### T·∫°o user service file

```bash
# T·∫°o systemd user directory
mkdir -p ~/.config/systemd/user

# T·∫°o service file
cat > ~/.config/systemd/user/ollama.service <<'EOF'
[Unit]
Description=Ollama Local LLM Server
Documentation=https://ollama.com/docs
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
ExecStart=/usr/local/bin/ollama serve
Environment="OLLAMA_HOST=127.0.0.1:11434"
Environment="OLLAMA_MODELS=/home/openclaw/.ollama/models"
Restart=on-failure
RestartSec=5
StandardOutput=journal
StandardError=journal

# Security hardening
NoNewPrivileges=true
PrivateTmp=true

[Install]
WantedBy=default.target
EOF
```

{{< callout type="warning" >}}
**Bind localhost only:** `OLLAMA_HOST=127.0.0.1` ƒë·∫£m b·∫£o Ollama ch·ªâ accessible t·ª´ localhost, kh√¥ng expose ra internet.
{{< /callout >}}

### Enable v√† start service

```bash
# Reload systemd daemon
systemctl --user daemon-reload

# Enable auto-start
systemctl --user enable ollama.service

# Start service
systemctl --user start ollama.service

# Check status
systemctl --user status ollama.service

# View logs
journalctl --user -u ollama.service -f
```

**Service ph·∫£i ·ªü tr·∫°ng th√°i `active (running)`:**

```
‚óè ollama.service - Ollama Local LLM Server
     Loaded: loaded (~/.config/systemd/user/ollama.service; enabled)
     Active: active (running) since Sat 2026-02-15 10:30:15 +07
```

### Enable linger (persistent user services)

```bash
# Cho ph√©p user services ch·∫°y khi user logout
sudo loginctl enable-linger openclaw

# Verify
loginctl show-user openclaw | grep Linger
```

## B∆∞·ªõc 4: C√†i ƒë·∫∑t OpenClaw

### Ch·∫°y official installer

```bash
# Download v√† ch·∫°y installer
curl -fsSL https://openclaw.ai/install.sh | bash

# Ho·∫∑c manual install via npm
# npm install -g @openclaw/cli
```

**Installer s·∫Ω:**

1. Install `@openclaw/cli` globally
2. T·∫°o directory `~/.openclaw/`
3. Generate initial config
4. Prompt onboarding wizard

### Ch·∫°y onboarding wizard

```bash
# Start interactive setup
openclaw onboard --install-daemon
```

**Wizard s·∫Ω h·ªèi:**

```
? Select installation mode:
  ‚ùØ Server daemon (recommended for VPS)
    Desktop app

? Choose workspace directory:
  ‚ùØ /home/openclaw/openclaw/workspace (default)
    Custom path...

? Enable web UI?
  ‚ùØ Yes, on port 18789
    No, CLI only

? Generate gateway token:
  ‚ùØ Auto-generate secure token
    Custom token...

? Install default skills?
  ‚ùØ Yes (git, file-manager, web-search, code-executor)
    Minimal (file-manager only)
```

**Ch·ªçn:**

- Server daemon mode
- Default workspace path
- Web UI on port 18789
- Auto-generate token
- Install default skills

{{< callout type="danger" >}}
**L∆∞u gateway token:** Token n√†y d√πng ƒë·ªÉ authenticate API requests. Copy v√† l∆∞u v√†o password manager ngay!
{{< /callout >}}

### Verify installation

```bash
# Check CLI version
openclaw --version

# Check config file
cat ~/.openclaw/openclaw.json

# List installed skills
openclaw skills list

# Check workspace
ls -la ~/openclaw/workspace/
```

## B∆∞·ªõc 5: C·∫•u h√¨nh LLM providers

### Edit openclaw.json

```bash
# Backup config
cp ~/.openclaw/openclaw.json ~/.openclaw/openclaw.json.bak

# Edit config
vim ~/.openclaw/openclaw.json
```

### Config v·ªõi Ollama primary + Anthropic fallback

```json
{
  "version": "1.0",
  "gateway": {
    "port": 18789,
    "host": "0.0.0.0",
    "token": "YOUR_GENERATED_TOKEN_HERE",
    "cors": {
      "enabled": true,
      "origins": ["https://your-domain.com"]
    }
  },
  "workspace": {
    "root": "/home/openclaw/openclaw/workspace",
    "maxSize": "10GB"
  },
  "llm": {
    "providers": [
      {
        "name": "ollama-primary",
        "type": "ollama",
        "enabled": true,
        "priority": 1,
        "config": {
          "baseUrl": "http://127.0.0.1:11434",
          "defaultModel": "qwen2.5:7b",
          "models": [
            {
              "id": "qwen2.5:7b",
              "contextWindow": 32768,
              "maxTokens": 4096,
              "capabilities": ["chat", "code", "reasoning"]
            },
            {
              "id": "llama3.1:8b",
              "contextWindow": 131072,
              "maxTokens": 8192,
              "capabilities": ["chat", "reasoning", "long-context"]
            }
          ],
          "timeout": 120000,
          "retries": 2
        }
      },
      {
        "name": "anthropic-fallback",
        "type": "anthropic",
        "enabled": false,
        "priority": 2,
        "config": {
          "apiKey": "${ANTHROPIC_API_KEY}",
          "defaultModel": "claude-3-5-sonnet-20241022",
          "models": [
            {
              "id": "claude-3-5-sonnet-20241022",
              "contextWindow": 200000,
              "maxTokens": 8192,
              "capabilities": ["chat", "code", "reasoning", "vision"]
            }
          ],
          "timeout": 60000
        }
      }
    ],
    "routing": {
      "strategy": "priority-fallback",
      "rules": [
        {
          "condition": "context_size > 32000",
          "provider": "ollama-primary",
          "model": "llama3.1:8b"
        },
        {
          "condition": "task_type == 'vision'",
          "provider": "anthropic-fallback"
        },
        {
          "condition": "ollama_failed",
          "provider": "anthropic-fallback"
        }
      ],
      "fallbackChain": [
        "ollama-primary",
        "anthropic-fallback"
      ]
    }
  },
  "skills": {
    "enabled": true,
    "autoLoad": true,
    "allowCustom": true,
    "sandboxMode": true
  },
  "logging": {
    "level": "info",
    "file": "/home/openclaw/.openclaw/logs/openclaw.log",
    "maxSize": "100MB",
    "maxFiles": 5
  },
  "security": {
    "rateLimit": {
      "enabled": true,
      "requests": 100,
      "window": "15m"
    },
    "allowedIPs": [],
    "blockedIPs": []
  }
}
```

### Th√™m Anthropic API key (n·∫øu d√πng)

```bash
# T·∫°o .env file
cat > ~/.openclaw/.env <<'EOF'
ANTHROPIC_API_KEY=sk-ant-xxx-your-api-key-here
EOF

# Secure permissions
chmod 600 ~/.openclaw/.env

# Load env vars cho session hi·ªán t·∫°i
export $(cat ~/.openclaw/.env | xargs)
```

{{< callout type="info" >}}
**Free tier Anthropic:** N·∫øu kh√¥ng c√≥ API key, set `enabled: false` cho anthropic provider. OpenClaw ch·ªâ d√πng Ollama local.
{{< /callout >}}

### Validate config

```bash
# Test config syntax
openclaw config validate

# Test LLM connectivity
openclaw llm test --provider ollama-primary
openclaw llm test --provider anthropic-fallback
```

**Output mong ƒë·ª£i:**

```
‚úì ollama-primary: Connected (qwen2.5:7b ready)
‚úì anthropic-fallback: Connected (claude-3-5-sonnet available)
```

## B∆∞·ªõc 6: T·∫°o OpenClaw systemd service

### T·∫°o service file

```bash
cat > ~/.config/systemd/user/openclaw.service <<'EOF'
[Unit]
Description=OpenClaw AI Assistant Gateway
Documentation=https://docs.openclaw.ai
After=network-online.target ollama.service
Wants=network-online.target
Requires=ollama.service

[Service]
Type=simple
WorkingDirectory=/home/openclaw
ExecStart=/usr/bin/openclaw daemon start
EnvironmentFile=/home/openclaw/.openclaw/.env
Restart=on-failure
RestartSec=10
StandardOutput=journal
StandardError=journal

# Security
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=read-only
ReadWritePaths=/home/openclaw/.openclaw /home/openclaw/openclaw/workspace

# Resource limits
LimitNOFILE=65536
MemoryMax=2G

[Install]
WantedBy=default.target
EOF
```

### Enable v√† start service

```bash
# Reload daemon
systemctl --user daemon-reload

# Enable auto-start
systemctl --user enable openclaw.service

# Start service
systemctl --user start openclaw.service

# Check status
systemctl --user status openclaw.service

# View real-time logs
journalctl --user -u openclaw.service -f
```

**Service ph·∫£i active:**

```
‚óè openclaw.service - OpenClaw AI Assistant Gateway
     Loaded: loaded (~/.config/systemd/user/openclaw.service; enabled)
     Active: active (running) since Sat 2026-02-15 11:05:42 +07
```

### Verify listening port

```bash
# Check port 18789 listening
ss -tlnp | grep 18789

# Test local access
curl http://localhost:18789/health
```

**Response mong ƒë·ª£i:**

```json
{
  "status": "healthy",
  "version": "1.0.0",
  "uptime": 127,
  "llm": {
    "ollama-primary": "connected",
    "anthropic-fallback": "connected"
  }
}
```

## B∆∞·ªõc 7: C·∫≠p nh·∫≠t Caddy / Web Access

### K·ªãch b·∫£n A: C√≥ domain v·ªõi Caddy

Caddyfile ƒë√£ ƒë∆∞·ª£c config ·ªü Ph·∫ßn 1. Verify reverse proxy ho·∫°t ƒë·ªông:

```bash
# Test qua domain (thay your-domain.com)
curl https://your-domain.com/health

# Ho·∫∑c test t·ª´ browser
# https://your-domain.com
```

**Web UI s·∫Ω load v·ªõi:**

- Login page y√™u c·∫ßu gateway token
- Dashboard sau khi authenticated
- Chat interface v·ªõi model selector

### K·ªãch b·∫£n B: Ch·ªâ c√≥ IP

**Option 1: Direct access (kh√¥ng b·∫£o m·∫≠t)**

```bash
# Allow port qua UFW (tr√™n VPS)
sudo ufw allow 18789/tcp comment 'OpenClaw Gateway'

# Test t·ª´ m√°y local
curl http://YOUR_VPS_IP:18789/health
```

{{< callout type="warning" >}}
**HTTP kh√¥ng m√£ h√≥a:** Gateway token s·∫Ω b·ªã l·ªô qua plaintext. Ch·ªâ d√πng cho testing!
{{< /callout >}}

**Option 2: SSH Tunnel (khuy·∫øn ngh·ªã)**

```bash
# T·ª´ m√°y local, t·∫°o tunnel
ssh -L 8789:localhost:18789 openclaw@YOUR_VPS_IP -N

# Access tr√™n browser local
# http://localhost:8789
```

**Option 3: Tailscale VPN (best practice)**

```bash
# C√†i Tailscale tr√™n VPS
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# Access qua Tailscale IP (100.x.y.z:18789)
# Encrypted, kh√¥ng c·∫ßn expose port
```

## B∆∞·ªõc 8: Verification & Testing

### Health checks

```bash
# Service status
systemctl --user status openclaw.service
systemctl --user status ollama.service

# Port listening
ss -tlnp | grep -E '18789|11434'

# Process running
ps aux | grep -E 'openclaw|ollama'

# Disk usage
du -sh ~/.openclaw
du -sh ~/.ollama/models
df -h ~/openclaw/workspace
```

### Functional tests

```bash
# Test CLI chat
openclaw chat "Write a bash script to backup /etc"

# Test API endpoint
curl -X POST http://localhost:18789/v1/chat \
  -H "Authorization: Bearer YOUR_GATEWAY_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:7b",
    "messages": [
      {"role": "user", "content": "Hello OpenClaw!"}
    ]
  }'

# Test skill execution
openclaw run git status --workspace ~/openclaw/workspace

# Test file operations
openclaw workspace create test-project
openclaw workspace list
```

### Web UI testing

1. **Login:** Navigate to `https://your-domain.com` (ho·∫∑c localhost:8789)
2. **Authenticate:** Enter gateway token from `openclaw.json`
3. **Test chat:** Send message "Analyze ~/openclaw/workspace structure"
4. **Check model:** Verify qwen2.5:7b ƒë∆∞·ª£c s·ª≠ d·ª•ng (hi·ªÉn th·ªã ·ªü header)
5. **Test skill:** Run `/skill git status` trong chat
6. **Check logs:** View real-time logs trong Settings panel

### Performance monitoring

```bash
# Check resource usage
htop

# Ollama GPU usage (n·∫øu c√≥ GPU)
ollama ps

# OpenClaw metrics
curl http://localhost:18789/metrics

# Log analysis
journalctl --user -u openclaw.service --since "10 minutes ago"
```

## Troubleshooting

### Ollama kh√¥ng start

```bash
# Check service logs
journalctl --user -u ollama.service -n 50

# Common issues:
# - Port 11434 ƒë√£ b·ªã d√πng: lsof -i :11434
# - Permission denied: chown -R openclaw ~/.ollama
# - Model corrupted: ollama pull qwen2.5:7b --force
```

### OpenClaw kh√¥ng k·∫øt n·ªëi Ollama

```bash
# Test Ollama connectivity
curl http://127.0.0.1:11434/api/tags

# Check firewall kh√¥ng block localhost
sudo iptables -L -n | grep 11434

# Verify config baseUrl
grep baseUrl ~/.openclaw/openclaw.json
```

### Web UI kh√¥ng load

```bash
# Check Caddy logs
sudo journalctl -u caddy -f

# Verify domain DNS
dig your-domain.com

# Test direct access
curl http://localhost:18789

# Check CORS settings trong openclaw.json
```

### High memory usage

```bash
# Check model loaded
ollama ps

# Unload unused models
ollama stop qwen2.5:7b

# Reduce concurrent requests
# Edit openclaw.json -> security.rateLimit
```

{{< callout type="tip" >}}
**Debug mode:** Start OpenClaw manually v·ªõi `openclaw daemon start --debug` ƒë·ªÉ xem chi ti·∫øt logs.
{{< /callout >}}

## T·ªïng k·∫øt

B·∫°n ƒë√£ deploy th√†nh c√¥ng OpenClaw AI assistant v·ªõi:

- ‚úÖ Ollama local LLM runtime v·ªõi 2+ models
- ‚úÖ OpenClaw gateway v·ªõi systemd auto-restart
- ‚úÖ Multi-provider routing (Ollama primary + Anthropic fallback)
- ‚úÖ Web UI access qua HTTPS/SSH tunnel
- ‚úÖ Rate limiting v√† security hardening
- ‚úÖ Workspace isolation v√† skill sandboxing

**Ki·∫øn tr√∫c ho√†n ch·ªânh:**

```
User ‚Üí Caddy HTTPS ‚Üí OpenClaw Gateway
                         ‚îú‚îÄ‚Üí Ollama (qwen2.5, llama3.1)
                         ‚îú‚îÄ‚Üí Anthropic API (fallback)
                         ‚îú‚îÄ‚Üí Skills (git, file, code-exec)
                         ‚îî‚îÄ‚Üí Workspace (isolated storage)
```

**ƒêi·ªÉm checklist:**

- [ ] Ollama service active v√† models loaded
- [ ] OpenClaw service running v·ªõi gateway token secured
- [ ] Web UI accessible qua domain ho·∫∑c tunnel
- [ ] LLM providers test passed
- [ ] Skills functional (git, file-manager tested)
- [ ] Logs monitoring setup
- [ ] Resource usage trong ng∆∞·ª°ng (< 80% RAM)

## B∆∞·ªõc ti·∫øp theo

Trong **Ph·∫ßn 3: T√≠ch h·ª£p k√™nh nh·∫Øn tin**, ch√∫ng ta s·∫Ω:

- T·∫°o Telegram bot v√† k·∫øt n·ªëi v·ªõi OpenClaw
- Setup Discord bot v·ªõi slash commands
- Integrate WhatsApp Business API (n√¢ng cao)
- C·∫•u h√¨nh multi-channel routing
- Implement conversation context persistence

Trong **Ph·∫ßn 4: TƒÉng c∆∞·ªùng b·∫£o m·∫≠t**, ch√∫ng ta s·∫Ω:

- Setup authentication layers (OAuth, API keys)
- Implement audit logging v√† intrusion detection
- Network isolation v·ªõi Podman containers
- Secret management v·ªõi Vault
- Compliance v√† GDPR considerations

H·∫πn g·∫∑p l·∫°i! ü§ñ
