---
title: "DOKS Mastery Ph·∫ßn 2: H∆∞·ªõng d·∫´n cho System Admin"
date: 2026-02-14
draft: false
description: "H∆∞·ªõng d·∫´n qu·∫£n l√Ω DOKS cluster cho System Admin - v√≤ng ƒë·ªùi cluster, node pool, gi√°m s√°t t√†i nguy√™n, sao l∆∞u & kh√¥i ph·ª•c"
categories: ["Kubernetes"]
tags: ["kubernetes", "digitalocean", "doks", "sysadmin", "cluster-management", "node-pool", "backup"]
series: ["DOKS Mastery"]
weight: 2
mermaid: true
---

## Gi·ªõi thi·ªáu

Trong chu·ªói b√†i **DOKS Mastery**, ph·∫ßn 2 n√†y t·∫≠p trung v√†o vai tr√≤ c·ªßa **System Administrator** - ng∆∞·ªùi ch·ªãu tr√°ch nhi·ªám qu·∫£n l√Ω v√≤ng ƒë·ªùi cluster, ƒë·∫£m b·∫£o hi·ªáu su·∫•t, t√≠nh s·∫µn s√†ng cao v√† kh·∫£ nƒÉng ph·ª•c h·ªìi th·∫£m h·ªça.

Kh√°c v·ªõi Developer t·∫≠p trung v√†o deploy ·ª©ng d·ª•ng, SysAdmin c·∫ßn n·∫Øm v·ªØng c√°c k·ªπ nƒÉng sau:

- **Qu·∫£n l√Ω Cluster lifecycle**: N√¢ng c·∫•p Kubernetes version, b·∫£o tr√¨ ƒë·ªãnh k·ª≥
- **Qu·∫£n l√Ω Node Pool**: Th√™m/x√≥a worker nodes, t·ª± ƒë·ªông m·ªü r·ªông (auto-scaling)
- **Gi√°m s√°t t√†i nguy√™n**: CPU, memory, disk, network c·ªßa to√†n cluster
- **Backup & Disaster Recovery**: Sao l∆∞u d·ªØ li·ªáu, kh√¥i ph·ª•c khi c√≥ s·ª± c·ªë

{{< callout type="info" >}}
**Y√™u c·∫ßu**: ƒê√£ ho√†n th√†nh [Ph·∫ßn 1: Chu·∫©n b·ªã Kubernetes tr√™n DigitalOcean]({{< relref "/posts/doks-mastery/01-doks-preparation" >}}) v√† c√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ Linux/CLI.
{{< /callout >}}

### Vai tr√≤ c·ªßa SysAdmin trong Kubernetes

{{< mermaid >}}
graph TB
    SysAdmin[System Administrator]

    SysAdmin --> Cluster[Qu·∫£n l√Ω Cluster]
    SysAdmin --> Nodes[Qu·∫£n l√Ω Node Pool]
    SysAdmin --> Monitor[Gi√°m s√°t & Alert]
    SysAdmin --> Backup[Backup & DR]

    Cluster --> Upgrade[N√¢ng c·∫•p K8s version]
    Cluster --> Maintain[B·∫£o tr√¨ ƒë·ªãnh k·ª≥]

    Nodes --> AutoScale[Auto-scaling]
    Nodes --> Resize[Thay ƒë·ªïi k√≠ch th∆∞·ªõc]

    Monitor --> Metrics[CPU/Memory/Disk]
    Monitor --> Quota[Resource Quotas]

    Backup --> Velero[Velero Backup]
    Backup --> Snapshots[Volume Snapshots]

    style SysAdmin fill:#ff6b6b
    style Cluster fill:#4ecdc4
    style Nodes fill:#45b7d1
    style Monitor fill:#96ceb4
    style Backup fill:#ffeaa7
{{< /mermaid >}}

SysAdmin ƒë√≥ng vai tr√≤ c·∫ßu n·ªëi gi·ªØa infrastructure v√† application layer, ƒë·∫£m b·∫£o m√¥i tr∆∞·ªùng Kubernetes ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v√† hi·ªáu qu·∫£.

---

## Qu·∫£n l√Ω v√≤ng ƒë·ªùi Cluster

### Li·ªát k√™ v√† xem th√¥ng tin Cluster

**CLI v·ªõi doctl:**

```bash
# Li·ªát k√™ t·∫•t c·∫£ clusters
doctl kubernetes cluster list

# Xem chi ti·∫øt m·ªôt cluster
doctl kubernetes cluster get <cluster-name>

# L·∫•y kubeconfig
doctl kubernetes cluster kubeconfig save <cluster-name>
```

**DigitalOcean Console:**

1. ƒêƒÉng nh·∫≠p [cloud.digitalocean.com](https://cloud.digitalocean.com)
2. V√†o **Kubernetes** t·ª´ menu b√™n tr√°i
3. Click v√†o t√™n cluster ƒë·ªÉ xem chi ti·∫øt
4. Tab **Overview** hi·ªÉn th·ªã: version, endpoint, node pools, resource usage

{{< callout type="tip" >}}
**Pro tip**: S·ª≠ d·ª•ng `doctl kubernetes cluster list --format Name,Region,Version,Status` ƒë·ªÉ xu·∫•t th√¥ng tin d·∫°ng b·∫£ng t√πy ch·ªânh.
{{< /callout >}}

### N√¢ng c·∫•p Kubernetes Version

DigitalOcean t·ª± ƒë·ªông cung c·∫•p c√°c b·∫£n v√° b·∫£o m·∫≠t, nh∆∞ng **major/minor version upgrade** c·∫ßn SysAdmin th·ª±c hi·ªán th·ªß c√¥ng.

**CLI v·ªõi doctl:**

```bash
# Ki·ªÉm tra version kh·∫£ d·ª•ng
doctl kubernetes options versions

# N√¢ng c·∫•p cluster l√™n version m·ªõi
doctl kubernetes cluster upgrade <cluster-name> --version <new-version>

# V√≠ d·ª•: N√¢ng c·∫•p t·ª´ 1.28 l√™n 1.29
doctl kubernetes cluster upgrade my-production-cluster --version 1.29.1-do.0
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster
2. Tab **Settings** ‚Üí **Upgrade** section
3. Ch·ªçn version m·ªõi t·ª´ dropdown
4. Click **Upgrade Cluster**
5. X√°c nh·∫≠n upgrade (cluster s·∫Ω upgrade t·ª´ng node m·ªôt, rolling restart)

{{< callout type="warning" >}}
**L∆∞u √Ω quan tr·ªçng khi upgrade**:
- Backup cluster tr∆∞·ªõc khi n√¢ng c·∫•p
- Ki·ªÉm tra [Kubernetes Release Notes](https://kubernetes.io/releases/) cho breaking changes
- Test tr√™n staging cluster tr∆∞·ªõc khi √°p d·ª•ng l√™n production
- Upgrade t·ª´ng minor version m·ªôt (1.28 ‚Üí 1.29 ‚Üí 1.30), kh√¥ng nh·∫£y c√≥c
{{< /callout >}}

### C·ª≠a s·ªï b·∫£o tr√¨ (Maintenance Windows)

DOKS t·ª± ƒë·ªông √°p d·ª•ng security patches trong **maintenance window** ƒë√£ c·∫•u h√¨nh.

**CLI v·ªõi doctl:**

```bash
# Xem maintenance window hi·ªán t·∫°i
doctl kubernetes cluster get <cluster-name> --format MaintenancePolicy

# C·∫≠p nh·∫≠t maintenance window (format: day=<day>,start_time=<HH:MM>)
doctl kubernetes cluster update <cluster-name> \
  --maintenance-window "day=sunday,start_time=02:00"
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster
2. Tab **Settings** ‚Üí **Maintenance** section
3. Ch·ªçn **Day of week** v√† **Start time** (UTC)
4. Click **Update**

{{< callout type="info" >}}
**Best Practice**: ƒê·∫∑t maintenance window v√†o th·ªùi gian traffic th·∫•p (2-4 AM UTC cho US traffic, t√πy timezone).
{{< /callout >}}

### X√≥a Cluster

**CLI v·ªõi doctl:**

```bash
# X√≥a cluster (c·∫ßn x√°c nh·∫≠n)
doctl kubernetes cluster delete <cluster-name>

# X√≥a k√®m theo Load Balancers v√† Volumes
doctl kubernetes cluster delete <cluster-name> --dangerous --update-kubeconfig
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster
2. Tab **Settings** ‚Üí Scroll xu·ªëng **Destroy** section
3. Click **Destroy** button
4. Nh·∫≠p t√™n cluster ƒë·ªÉ x√°c nh·∫≠n

{{< callout type="warning" >}}
**C·∫£nh b√°o**: X√≥a cluster s·∫Ω x√≥a t·∫•t c·∫£ resources (pods, services, volumes). Load Balancers v√† Persistent Volumes **kh√¥ng t·ª± ƒë·ªông x√≥a** ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu. X√≥a th·ªß c√¥ng sau khi ki·ªÉm tra.
{{< /callout >}}

---

## Qu·∫£n l√Ω Node Pool

Node Pool l√† nh√≥m c√°c worker nodes c√≥ c√πng c·∫•u h√¨nh (CPU, RAM, disk). DOKS cho ph√©p t·∫°o nhi·ªÅu node pools v·ªõi c·∫•u h√¨nh kh√°c nhau cho workloads kh√°c nhau.

### Th√™m Node Pool m·ªõi

**CLI v·ªõi doctl:**

```bash
# T·∫°o node pool m·ªõi
doctl kubernetes cluster node-pool create <cluster-name> \
  --name <pool-name> \
  --size <droplet-size> \
  --count <node-count> \
  --auto-scale \
  --min-nodes <min> \
  --max-nodes <max>

# V√≠ d·ª•: T·∫°o pool cho workload CPU-intensive
doctl kubernetes cluster node-pool create my-cluster \
  --name cpu-optimized-pool \
  --size c-4 \
  --count 2 \
  --auto-scale \
  --min-nodes 2 \
  --max-nodes 5
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster
2. Tab **Overview** ‚Üí Scroll xu·ªëng **Node Pools** section
3. Click **Add Node Pool**
4. Ch·ªçn:
   - **Node plan** (Droplet size)
   - **Node count** (s·ªë l∆∞·ª£ng ban ƒë·∫ßu)
   - **Enable auto-scaling** (optional)
   - **Min/Max nodes** (n·∫øu b·∫≠t auto-scale)
   - **Tags** v√† **Labels** (optional, ƒë·ªÉ organize)
5. Click **Add Node Pool**

{{< callout type="tip" >}}
**Use case cho multiple node pools**:
- **General pool** (`s-2vcpu-4gb`): Web apps, API servers
- **CPU pool** (`c-4`): CI/CD runners, build jobs
- **Memory pool** (`m-4vcpu-32gb`): Databases, caching layers
- **GPU pool** (n·∫øu c·∫ßn): ML/AI workloads
{{< /callout >}}

### Thay ƒë·ªïi k√≠ch th∆∞·ªõc Node Pool

**CLI v·ªõi doctl:**

```bash
# Resize node pool (tƒÉng/gi·∫£m s·ªë l∆∞·ª£ng nodes)
doctl kubernetes cluster node-pool update <cluster-name> <pool-id> \
  --count <new-count>

# V√≠ d·ª•: TƒÉng t·ª´ 2 l√™n 4 nodes
doctl kubernetes cluster node-pool update my-cluster pool-abc123 --count 4

# List node pools ƒë·ªÉ l·∫•y pool-id
doctl kubernetes cluster node-pool list <cluster-name>
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster ‚Üí Tab **Overview**
2. T√¨m node pool c·∫ßn resize trong **Node Pools** section
3. Click **Edit** (icon b√∫t ch√¨)
4. ƒêi·ªÅu ch·ªânh **Node Count** slider
5. Click **Update**

{{< callout type="info" >}}
**L∆∞u √Ω**: Khi gi·∫£m s·ªë nodes, Kubernetes s·∫Ω drain nodes (di chuy·ªÉn pods sang nodes kh√°c) tr∆∞·ªõc khi x√≥a. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t.
{{< /callout >}}

### C·∫•u h√¨nh Auto-Scaling

Auto-scaling t·ª± ƒë·ªông th√™m/b·ªõt nodes d·ª±a tr√™n resource usage (CPU/memory requests).

**CLI v·ªõi doctl:**

```bash
# Enable auto-scaling cho node pool hi·ªán c√≥
doctl kubernetes cluster node-pool update <cluster-name> <pool-id> \
  --auto-scale \
  --min-nodes 2 \
  --max-nodes 10

# Disable auto-scaling
doctl kubernetes cluster node-pool update <cluster-name> <pool-id> \
  --auto-scale=false
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster ‚Üí Tab **Overview**
2. T√¨m node pool ‚Üí Click **Edit**
3. B·∫≠t **Autoscale** toggle
4. ƒê·∫∑t **Min nodes** v√† **Max nodes**
5. Click **Update**

{{< mermaid >}}
graph LR
    A[Pod Scheduling Failed] --> B{Resource Available?}
    B -->|No| C[Cluster Autoscaler Triggered]
    C --> D[Add New Node]
    D --> E[Pod Scheduled Successfully]

    F[Node Underutilized] --> G{Pod Count Low?}
    G -->|Yes| H[Drain Node]
    H --> I[Remove Node]
    I --> J[Cost Optimized]

    style A fill:#ff6b6b
    style E fill:#51cf66
    style J fill:#51cf66
    style C fill:#ffd43b
    style H fill:#ffd43b
{{< /mermaid >}}

{{< callout type="warning" >}}
**Auto-scaling considerations**:
- Set `min-nodes` ƒë·ªß cao ƒë·ªÉ handle baseline traffic
- Set `max-nodes` ƒë·ªÉ tr√°nh chi ph√≠ v∆∞·ª£t ng√¢n s√°ch
- Auto-scaling d·ª±a tr√™n **pod requests**, kh√¥ng ph·∫£i actual usage
- Th·ªùi gian scale-up: 1-3 ph√∫t (th·ªùi gian kh·ªüi ƒë·ªông node m·ªõi)
{{< /callout >}}

### X√≥a Node Pool

**CLI v·ªõi doctl:**

```bash
# X√≥a node pool
doctl kubernetes cluster node-pool delete <cluster-name> <pool-id>
```

**DigitalOcean Console:**

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster ‚Üí Tab **Overview**
2. T√¨m node pool ‚Üí Click **Delete** (icon th√πng r√°c)
3. X√°c nh·∫≠n x√≥a

{{< callout type="warning" >}}
**Ch√∫ √Ω**: Pods ƒëang ch·∫°y tr√™n node pool s·∫Ω b·ªã evict. ƒê·∫£m b·∫£o c√≥ node pool kh√°c ƒë·ªÉ Kubernetes reschedule pods.
{{< /callout >}}

---

## Gi√°m s√°t t√†i nguy√™n

### Gi√°m s√°t Node v√† Pod v·ªõi kubectl

**Xem resource usage c·ªßa nodes:**

```bash
# CPU v√† memory usage c·ªßa t·∫•t c·∫£ nodes
kubectl top nodes

# Output:
# NAME                   CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# pool-abc-123           250m         12%    1024Mi          25%
# pool-abc-456           800m         40%    3072Mi          75%
```

**Xem resource usage c·ªßa pods:**

```bash
# T·∫•t c·∫£ pods trong namespace
kubectl top pods -n <namespace>

# T·∫•t c·∫£ pods trong cluster
kubectl top pods --all-namespaces

# Sort theo CPU/memory
kubectl top pods --all-namespaces --sort-by=cpu
kubectl top pods --all-namespaces --sort-by=memory
```

{{< callout type="tip" >}}
**C√†i ƒë·∫∑t metrics-server** (DOKS ƒë√£ c√†i s·∫µn):
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```
{{< /callout >}}

### DigitalOcean Console Monitoring Dashboard

1. V√†o **Kubernetes** ‚Üí Ch·ªçn cluster
2. Tab **Insights** hi·ªÉn th·ªã:
   - **CPU Usage**: % CPU usage theo th·ªùi gian
   - **Memory Usage**: % memory usage theo th·ªùi gian
   - **Network I/O**: Bandwidth in/out
   - **Disk I/O**: Read/write operations
3. Click **View Details** ƒë·ªÉ drill down v√†o t·ª´ng node

{{< callout type="info" >}}
**DO Monitoring** t√≠ch h·ª£p s·∫µn v·ªõi DOKS, kh√¥ng c·∫ßn c√†i th√™m agent. Metrics l∆∞u tr·ªØ 14 ng√†y.
{{< /callout >}}

### Resource Quotas per Namespace

Resource Quotas gi·ªõi h·∫°n t·ªïng t√†i nguy√™n m√† m·ªôt namespace c√≥ th·ªÉ s·ª≠ d·ª•ng.

**T·∫°o ResourceQuota:**

```yaml
# resource-quota-dev.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"          # T·ªïng CPU requests: 10 cores
    requests.memory: 20Gi       # T·ªïng memory requests: 20GB
    limits.cpu: "20"            # T·ªïng CPU limits: 20 cores
    limits.memory: 40Gi         # T·ªïng memory limits: 40GB
    persistentvolumeclaims: "10" # T·ªëi ƒëa 10 PVCs
    pods: "50"                  # T·ªëi ƒëa 50 pods
    services.loadbalancers: "2" # T·ªëi ƒëa 2 LoadBalancers
```

**Apply quota:**

```bash
kubectl apply -f resource-quota-dev.yaml

# Xem quota status
kubectl describe resourcequota dev-quota -n development
```

**Output:**

```text
Name:                   dev-quota
Namespace:              development
Resource                Used   Hard
--------                ----   ----
limits.cpu              5      20
limits.memory           10Gi   40Gi
persistentvolumeclaims  3      10
pods                    12     50
requests.cpu            2.5    10
requests.memory         5Gi    20Gi
services.loadbalancers  1      2
```

{{< callout type="warning" >}}
**Quan tr·ªçng**: N·∫øu namespace c√≥ ResourceQuota, **t·∫•t c·∫£ pods ph·∫£i khai b√°o resource requests/limits**. N·∫øu kh√¥ng, pod s·∫Ω b·ªã reject.
{{< /callout >}}

### LimitRange Setup

LimitRange ƒë·∫∑t gi·ªõi h·∫°n m·∫∑c ƒë·ªãnh cho t·ª´ng container/pod trong namespace.

**T·∫°o LimitRange:**

```yaml
# limit-range-dev.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: development
spec:
  limits:
  # Gi·ªõi h·∫°n cho containers
  - type: Container
    max:
      cpu: "2"           # Max 2 cores per container
      memory: 4Gi        # Max 4GB per container
    min:
      cpu: 100m          # Min 100m CPU per container
      memory: 128Mi      # Min 128MB per container
    default:
      cpu: 500m          # Default limit n·∫øu kh√¥ng khai b√°o
      memory: 1Gi
    defaultRequest:
      cpu: 200m          # Default request n·∫øu kh√¥ng khai b√°o
      memory: 512Mi
    maxLimitRequestRatio:
      cpu: "4"           # Limit/Request ratio t·ªëi ƒëa 4x
      memory: "2"        # Limit/Request ratio t·ªëi ƒëa 2x

  # Gi·ªõi h·∫°n cho pods
  - type: Pod
    max:
      cpu: "4"           # Max 4 cores per pod
      memory: 8Gi        # Max 8GB per pod

  # Gi·ªõi h·∫°n cho PVCs
  - type: PersistentVolumeClaim
    max:
      storage: 50Gi      # Max 50GB per PVC
    min:
      storage: 1Gi       # Min 1GB per PVC
```

**Apply LimitRange:**

```bash
kubectl apply -f limit-range-dev.yaml

# Xem LimitRange
kubectl describe limitrange dev-limits -n development
```

{{< callout type="tip" >}}
**Best Practice**: K·∫øt h·ª£p ResourceQuota (gi·ªõi h·∫°n namespace) v√† LimitRange (gi·ªõi h·∫°n container) ƒë·ªÉ ki·ªÉm so√°t t√†i nguy√™n hi·ªáu qu·∫£.
{{< /callout >}}

### V√≠ d·ª• t·ªïng h·ª£p: Namespace v·ªõi Quota v√† Limits

```bash
# 1. T·∫°o namespace
kubectl create namespace production

# 2. Apply ResourceQuota
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
    pods: "200"
EOF

# 3. Apply LimitRange
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limits
  namespace: production
spec:
  limits:
  - type: Container
    max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 100m
      memory: 128Mi
    default:
      cpu: 1
      memory: 2Gi
    defaultRequest:
      cpu: 500m
      memory: 1Gi
EOF

# 4. Verify
kubectl get resourcequota,limitrange -n production
```

---

## Sao l∆∞u & Kh√¥i ph·ª•c (Backup & DR)

### Velero - Backup solution cho Kubernetes

**Velero** l√† tool open-source ƒë·ªÉ backup/restore to√†n b·ªô Kubernetes resources v√† Persistent Volumes.

**C√†i ƒë·∫∑t Velero:**

```bash
# 1. Download Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xvf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# 2. T·∫°o DigitalOcean Spaces (S3-compatible storage) ƒë·ªÉ l∆∞u backup
# V√†o DO Console ‚Üí Spaces ‚Üí Create Space
# L∆∞u Access Key v√† Secret Key

# 3. T·∫°o credentials file
cat <<EOF > credentials-velero
[default]
aws_access_key_id=<YOUR_SPACES_ACCESS_KEY>
aws_secret_access_key=<YOUR_SPACES_SECRET_KEY>
EOF

# 4. Install Velero v√†o cluster
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket <your-spaces-bucket-name> \
  --backup-location-config region=nyc3,s3ForcePathStyle="true",s3Url=https://nyc3.digitaloceanspaces.com \
  --snapshot-location-config region=nyc3 \
  --secret-file ./credentials-velero

# 5. Verify installation
kubectl get pods -n velero
```

{{< callout type="info" >}}
**DigitalOcean Spaces**: D·ªãch v·ª• object storage t∆∞∆°ng th√≠ch S3 API c·ªßa DO, gi√° $5/month cho 250GB.
{{< /callout >}}

**T·∫°o backup:**

```bash
# Backup to√†n b·ªô cluster
velero backup create full-cluster-backup

# Backup m·ªôt namespace
velero backup create app-backup --include-namespaces production

# Backup v·ªõi schedule (cron format)
velero schedule create daily-backup --schedule="0 2 * * *"

# Xem backup status
velero backup describe full-cluster-backup
velero backup logs full-cluster-backup
```

**Restore t·ª´ backup:**

```bash
# List backups
velero backup get

# Restore to√†n b·ªô backup
velero restore create --from-backup full-cluster-backup

# Restore ch·ªâ m·ªôt namespace
velero restore create --from-backup app-backup --include-namespaces production

# Xem restore status
velero restore describe <restore-name>
velero restore logs <restore-name>
```

{{< callout type="warning" >}}
**Velero limitations**:
- Kh√¥ng backup cluster-level configs (RBAC, CRDs) m·∫∑c ƒë·ªãnh ‚Üí C·∫ßn flag `--include-cluster-resources=true`
- Persistent Volumes c·∫ßn snapshot provider (DO Volumes h·ªó tr·ª£)
- Restore kh√¥ng t·ª± ƒë·ªông x√≥a resources ƒë√£ t·ªìn t·∫°i ‚Üí C·∫ßn cleanup tr∆∞·ªõc khi restore
{{< /callout >}}

### Persistent Volume Snapshots v·ªõi doctl

**T·∫°o snapshot c·ªßa Volume:**

```bash
# 1. List volumes
doctl compute volume list

# 2. T·∫°o snapshot
doctl compute volume snapshot <volume-id> --snapshot-name backup-2026-02-14

# 3. List snapshots
doctl compute volume-snapshot list
```

**DigitalOcean Console:**

1. V√†o **Volumes** t·ª´ menu b√™n tr√°i
2. T√¨m volume c·∫ßn backup ‚Üí Click **More** ‚Üí **Take Snapshot**
3. Nh·∫≠p t√™n snapshot
4. Click **Take Snapshot**

**Restore t·ª´ snapshot:**

```bash
# T·∫°o volume m·ªõi t·ª´ snapshot
doctl compute volume create restored-volume \
  --size 100GB \
  --region nyc3 \
  --snapshot <snapshot-id>
```

**DigitalOcean Console:**

1. V√†o **Volumes** ‚Üí **Create Volume**
2. Ch·ªçn **From Snapshot**
3. Ch·ªçn snapshot t·ª´ dropdown
4. C·∫•u h√¨nh volume name, region
5. Click **Create Volume**

{{< callout type="tip" >}}
**Best Practice**: Combine Velero (Kubernetes resources) + DO Volume Snapshots (Persistent data) cho full disaster recovery.
{{< /callout >}}

### Backup Strategy cho Production

{{< callout type="info" >}}
**Recommended backup strategy**:

**Daily backups**:
- Velero scheduled backup: `0 2 * * *` (2 AM UTC m·ªói ng√†y)
- Backup to√†n b·ªô cluster resources
- Retention: 7 ng√†y

**Weekly backups**:
- Velero scheduled backup: `0 3 * * 0` (3 AM UTC m·ªói Ch·ªß nh·∫≠t)
- Backup to√†n b·ªô cluster + PV snapshots
- Retention: 4 tu·∫ßn

**Monthly backups**:
- Manual backup v√†o ng√†y 1 h√†ng th√°ng
- Backup to√†n b·ªô cluster + config exports
- Retention: 12 th√°ng

**Disaster Recovery Test**:
- M·ªói qu√Ω, test restore t·ª´ backup v√†o staging cluster
- Verify data integrity v√† application functionality
{{< /callout >}}

**Setup automated backups v·ªõi Velero:**

```bash
# Daily backup (gi·ªØ 7 ng√†y)
velero schedule create daily-backup \
  --schedule="0 2 * * *" \
  --ttl 168h

# Weekly backup (gi·ªØ 4 tu·∫ßn)
velero schedule create weekly-backup \
  --schedule="0 3 * * 0" \
  --ttl 672h \
  --include-cluster-resources=true

# Verify schedules
velero schedule get
```

---

## T·ªïng k·∫øt & B∆∞·ªõc ti·∫øp theo

Trong b√†i vi·∫øt n√†y, ch√∫ng ta ƒë√£ t√¨m hi·ªÉu c√°c k·ªπ nƒÉng c·∫ßn thi·∫øt cho **System Admin** qu·∫£n l√Ω DOKS cluster:

### Ki·∫øn th·ª©c ƒë√£ h·ªçc

‚úÖ **Qu·∫£n l√Ω Cluster Lifecycle**:
- List, get, upgrade, delete clusters
- C·∫•u h√¨nh maintenance windows
- Best practices khi n√¢ng c·∫•p Kubernetes version

‚úÖ **Qu·∫£n l√Ω Node Pool**:
- Th√™m/x√≥a/resize node pools
- C·∫•u h√¨nh auto-scaling
- Multiple node pools cho workload kh√°c nhau

‚úÖ **Gi√°m s√°t T√†i nguy√™n**:
- `kubectl top nodes/pods`
- ResourceQuota v√† LimitRange
- DigitalOcean monitoring dashboard

‚úÖ **Backup & Disaster Recovery**:
- Velero backup/restore
- DO Volume snapshots
- Backup strategy cho production

### Checklist cho SysAdmin

Sau khi ƒë·ªçc xong b√†i vi·∫øt, b·∫°n n√™n c√≥ kh·∫£ nƒÉng:

- [ ] N√¢ng c·∫•p Kubernetes cluster l√™n version m·ªõi
- [ ] T·∫°o v√† qu·∫£n l√Ω multiple node pools
- [ ] C·∫•u h√¨nh auto-scaling cho node pools
- [ ] Thi·∫øt l·∫≠p ResourceQuota v√† LimitRange cho namespaces
- [ ] Gi√°m s√°t resource usage v·ªõi kubectl v√† DO Console
- [ ] C√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng Velero backup
- [ ] T·∫°o v√† restore t·ª´ volume snapshots
- [ ] Thi·∫øt l·∫≠p automated backup schedules

### B∆∞·ªõc ti·∫øp theo

üöÄ **[Ph·∫ßn 3: H∆∞·ªõng d·∫´n cho DevOps Engineer]({{< relref "/posts/doks-mastery/03-devops-guide" >}})**

Trong ph·∫ßn ti·∫øp theo, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu:
- CI/CD pipelines v·ªõi GitHub Actions
- GitOps v·ªõi ArgoCD/FluxCD
- Advanced networking: Ingress, Service Mesh
- Security best practices: RBAC, Network Policies, Pod Security Standards
- Cost optimization strategies

{{< callout type="tip" >}}
**Th·ª±c h√†nh ngay**: T·∫°o m·ªôt DOKS cluster, deploy m·ªôt ·ª©ng d·ª•ng, thi·∫øt l·∫≠p auto-scaling, t·∫°o backup v·ªõi Velero. Hands-on l√† c√°ch h·ªçc t·ªët nh·∫•t!
{{< /callout >}}

---

### T√†i li·ªáu tham kh·∫£o

- [DigitalOcean Kubernetes Documentation](https://docs.digitalocean.com/products/kubernetes/)
- [Velero Documentation](https://velero.io/docs/)
- [Kubernetes Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)

---

*N·∫øu b·∫°n c√≥ c√¢u h·ªèi ho·∫∑c g·∫∑p v·∫•n ƒë·ªÅ khi th·ª±c h√†nh, h√£y ƒë·ªÉ l·∫°i comment b√™n d∆∞·ªõi!*
